{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef678f",
   "metadata": {},
   "source": [
    "# Face detection (light)\n",
    "\n",
    "• Blaze Face (https://arxiv.org/pdf/1907.05047)\n",
    "• Swift Face (https://www.researchgate.net/publication/344422660_SwiftFace_Real-Time_Face_Detection)\n",
    "• Yolo5Face (https://arxiv.org/pdf/2105.12931)\n",
    "• UltraFace (https://ieeexplore.ieee.org/document/10763901)\n",
    "\n",
    "git :\n",
    "https://github.com/vincent1bt/blazeface-tensorflow/blob/main/model_blocks.py\n",
    "\n",
    "papers:\n",
    "https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
    "https://fr.wikipedia.org/wiki/Max_pooling#:~:text=En%20apprentissage%20automatique%2C%20la%20couche,par%20un%20réseau%20de%20neurones.\n",
    "https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector#blazeface_short-range"
   ]
  },
  {
   "cell_type": "code",
   "id": "72d7e36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:46:58.886748Z",
     "start_time": "2026-01-07T16:46:55.825875Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from anyio.lowlevel import checkpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.util.tf_export import kwarg_only\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "print(tf.__version__, tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0 []\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4268d0e5",
   "metadata": {},
   "source": [
    "# Block"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb3c57a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:46:58.910304Z",
     "start_time": "2026-01-07T16:46:58.902214Z"
    }
   },
   "source": [
    "class BlazeBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, use_pool=True, **kwargs):\n",
    "        super(BlazeBlock, self).__init__(**kwargs)\n",
    "        self.strides = strides\n",
    "        self.filters = filters\n",
    "        self.use_pool = use_pool\n",
    "\n",
    "        if use_pool:\n",
    "            self.skip = layers.MaxPool2D(pool_size=2, strides=strides, padding='same')\n",
    "            self.channel_pad = layers.Lambda(lambda x: self._pad_channels(x, filters))\n",
    "\n",
    "        self.dw_conv = layers.DepthwiseConv2D((5, 5), strides=strides, padding='same')\n",
    "        self.conv = layers.Conv2D(filters, (1, 1), strides=(1, 1))\n",
    "\n",
    "        self.norm_1 = layers.BatchNormalization()\n",
    "        self.norm_2 = layers.BatchNormalization()\n",
    "\n",
    "        self.activation = layers.ReLU()\n",
    "\n",
    "    def _pad_channels(self, x, target_channels):\n",
    "        current_channels = x.shape[-1]\n",
    "        if current_channels is None:\n",
    "            return x\n",
    "\n",
    "        channels_to_add = target_channels - current_channels\n",
    "        if channels_to_add <= 0:\n",
    "            return x\n",
    "\n",
    "        paddings = [[0, 0], [0, 0], [0, 0], [0, channels_to_add]]\n",
    "        return tf.pad(x, paddings, mode='CONSTANT', constant_values=0)\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dw_conv(inputs)\n",
    "        x = self.norm_1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.norm_2(x)\n",
    "\n",
    "        if self.use_pool:\n",
    "            skip = self.skip(inputs)\n",
    "            skip = self.channel_pad(skip)\n",
    "            x = x + skip\n",
    "\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class DoubleBlazeBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides, use_pool = True, **kwargs):\n",
    "        super(DoubleBlazeBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.use_pool = use_pool\n",
    "\n",
    "        if use_pool:\n",
    "            self.skip = layers.MaxPool2D(pool_size=2, strides=strides, padding='same')\n",
    "            self.channel_pad = layers.Lambda(lambda x: self._pad_channels(x, filters))\n",
    "        \n",
    "        self.dw_conv_1 = layers.DepthwiseConv2D((5, 5), strides=strides, padding='same')\n",
    "        self.dw_conv_2 = layers.DepthwiseConv2D((5, 5), strides=(1, 1), padding='same')\n",
    "\n",
    "        self.conv_project = layers.Conv2D(self.filters, (1, 1), strides=(1, 1))\n",
    "        self.conv_expand = layers.Conv2D(self.filters, (1, 1), strides=(1, 1))\n",
    "\n",
    "        self.norm_1 = layers.BatchNormalization()\n",
    "        self.norm_2 = layers.BatchNormalization()\n",
    "        self.norm_3 = layers.BatchNormalization()\n",
    "        self.norm_4 = layers.BatchNormalization()\n",
    "\n",
    "        self.activation_1 = layers.ReLU()\n",
    "        self.activation_2 = layers.ReLU()\n",
    "\n",
    "    def _pad_channels(self, x, target_channels):\n",
    "        current_channels = x.shape[-1]\n",
    "        if current_channels is None:\n",
    "            return x\n",
    "\n",
    "        channels_to_add = target_channels - current_channels\n",
    "        if channels_to_add <= 0:\n",
    "            return x\n",
    "\n",
    "        paddings = [[0, 0], [0, 0], [0, 0], [0, channels_to_add]]\n",
    "        return tf.pad(x, paddings, mode='CONSTANT', constant_values=0)\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.dw_conv_1(inputs)\n",
    "        x = self.norm_1(x)\n",
    "        x = self.conv_project(x)\n",
    "        x = self.norm_2(x)\n",
    "\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        x = self.dw_conv_2(x)\n",
    "        x = self.norm_3(x)\n",
    "        x = self.conv_expand(x)\n",
    "        x = self.norm_4(x)\n",
    "\n",
    "        if self.use_pool:\n",
    "            skip = self.skip(inputs)\n",
    "            skip = self.channel_pad(skip)\n",
    "            x = x + skip\n",
    "\n",
    "        x = self.activation_2(x)\n",
    "\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7ed4489d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "aedaf2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:46:58.919218Z",
     "start_time": "2026-01-07T16:46:58.913938Z"
    }
   },
   "source": [
    "class BlazeModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BlazeModel, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv = layers.Conv2D(24, (5, 5), strides=2, padding='same')\n",
    "        self.activation = layers.ReLU()\n",
    "\n",
    "        self.block1 = BlazeBlock(filters=24, strides=1)\n",
    "        self.block2 = BlazeBlock(filters=24, strides=1)\n",
    "        self.block3 = BlazeBlock(filters=48, strides=2)\n",
    "        self.block4 = BlazeBlock(filters=48, strides=1)\n",
    "        self.block5 = BlazeBlock(filters=48, strides=1)\n",
    "        \n",
    "        self.block6 = DoubleBlazeBlock(filters=96, strides=2)\n",
    "        self.block7 = DoubleBlazeBlock(filters=96, strides=1)\n",
    "        self.block8 = DoubleBlazeBlock(filters=96, strides=1)\n",
    "        self.block9 = DoubleBlazeBlock(filters=96, strides=2)\n",
    "        self.block10 = DoubleBlazeBlock(filters=96, strides=1)\n",
    "        self.block11 = DoubleBlazeBlock(filters=96, strides=1)\n",
    "\n",
    "        self.classifier_8 = layers.Conv2D(2, (1, 1), strides=(1, 1), activation='sigmoid')\n",
    "        self.classifier_16 = layers.Conv2D(6, (1, 1), strides=(1, 1), activation='sigmoid')\n",
    "\n",
    "        self.regressor_8 = layers.Conv2D(8, (1, 1), strides=(1, 1))\n",
    "        self.regressor_16 = layers.Conv2D(24, (1, 1), strides=(1, 1))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # tf.print(\"Input size conv:\", tf.shape(inputs))\n",
    "        x = self.conv(inputs)\n",
    "        # tf.print(\"Input size ReLu:\", tf.shape(x))\n",
    "        x = self.activation(x)\n",
    "\n",
    "\n",
    "        # tf.print(\"Input size Single BlazeBlock_1:\", tf.shape(x))\n",
    "        x = self.block1(x)\n",
    "        # tf.print(\"Input size Single BlazeBlock_2:\", tf.shape(x))\n",
    "        x = self.block2(x)\n",
    "        # tf.print(\"Input size Single BlazeBlock_3:\", tf.shape(x))\n",
    "        x = self.block3(x)\n",
    "        # tf.print(\"Input size Single BlazeBlock_4:\", tf.shape(x))\n",
    "        x = self.block4(x)\n",
    "        # tf.print(\"Input size Single BlazeBlock_5:\", tf.shape(x))\n",
    "        x = self.block5(x)\n",
    "\n",
    "        # tf.print(\"Input size Double BlazeBlock_1:\", tf.shape(x))\n",
    "        x = self.block6(x)\n",
    "        # tf.print(\"Input size Double BlazeBlock_2:\", tf.shape(x))\n",
    "        x = self.block7(x)\n",
    "        # tf.print(\"Input size Double BlazeBlock_3:\", tf.shape(x))\n",
    "        x = self.block8(x)\n",
    "        # tf.print(\"Input size Double BlazeBlock_4:\", tf.shape(x))\n",
    "        h = self.block9(x)\n",
    "        # tf.print(\"Input size Double BlazeBlock_5:\", tf.shape(h))\n",
    "        h = self.block10(h)\n",
    "        # tf.print(\"Input size Double BlazeBlock_6:\", tf.shape(h))\n",
    "        h = self.block11(h)\n",
    "\n",
    "        # tf.print(\"Input size classifier_8:\", tf.shape(x))\n",
    "        c1 = self.classifier_8(x)\n",
    "        # tf.print(\"Input size reshape:\", tf.shape(c1))\n",
    "        c1 = layers.Reshape((-1, 1))(c1)\n",
    "\n",
    "        # tf.print(\"Input size classifier_16:\", tf.shape(h))\n",
    "        c2 = self.classifier_16(h)\n",
    "        # tf.print(\"Input size reshape:\", tf.shape(c2))\n",
    "        c2 = layers.Reshape((-1, 1))(c2)\n",
    "\n",
    "        c = layers.concatenate([c1, c2], axis = 1)\n",
    "        # tf.print(\"Output size classifier_concat:\", tf.shape(c))\n",
    "\n",
    "        # tf.print(\"Input size regressor_8:\", tf.shape(x))\n",
    "        r1 = self.regressor_8(x)\n",
    "        # tf.print(\"Input size reshape:\", tf.shape(r1))\n",
    "        r1 = layers.Reshape((-1, 4))(r1)\n",
    "\n",
    "        # tf.print(\"Input size regressor_16:\", tf.shape(h))\n",
    "        r2 = self.regressor_16(h)\n",
    "        # tf.print(\"Input size reshape:\", tf.shape(r2))\n",
    "        r2 = layers.Reshape((-1, 4))(r2)\n",
    "\n",
    "        r = layers.concatenate([r1, r2], axis=1)\n",
    "        # tf.print(\"Output size regressor_concat:\", tf.shape(r))\n",
    "\n",
    "        return c, r\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "69bcec18",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "id": "a5a1384d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:46:58.927395Z",
     "start_time": "2026-01-07T16:46:58.923136Z"
    }
   },
   "source": [
    "huber_loss = tf.keras.losses.Huber()\n",
    "\n",
    "def compute_loss(class_prediction, anchor_predictions, big_anchors, small_anchors, reference_anchors, ratio=3, scale=128):\n",
    "    B = big_anchors.shape[0]\n",
    "    list_big_anchors = tf.reshape(big_anchors, (B, -1, 5))\n",
    "\n",
    "    list_small_anchors = tf.reshape(small_anchors, (B, -1, 5))\n",
    "\n",
    "    list_true_anchors = tf.concat([list_small_anchors, list_big_anchors], axis=1)\n",
    "\n",
    "    true_classes = list_true_anchors[:, :, 0]\n",
    "    true_coords = list_true_anchors[:, :, 1:]\n",
    "\n",
    "    faces_mask_bool = tf.dtypes.cast(true_classes, tf.bool)\n",
    "\n",
    "    faces_num = tf.keras.backend.sum(true_classes)\n",
    "    background_num = int(faces_num * ratio)\n",
    "\n",
    "    class_predictions = tf.squeeze(class_prediction, axis=-1)\n",
    "\n",
    "    predicted_classes_scores = tf.where(faces_mask_bool, -99.0, class_predictions)\n",
    "\n",
    "    background_class_predictions = tf.sort(predicted_classes_scores, axis=-1, direction='DESCENDING')[:, :background_num]\n",
    "    positive_class_predictions = tf.boolean_mask(class_predictions, faces_mask_bool)\n",
    "\n",
    "    background_loss = tf.math.reduce_mean(tf.keras.losses.binary_crossentropy(tf.zeros_like(background_class_predictions), background_class_predictions))\n",
    "    positive_loss = tf.math.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(positive_class_predictions), positive_class_predictions))\n",
    "\n",
    "\n",
    "    x_center = reference_anchors[:, 0:1] + (anchor_predictions[..., 0:1] / scale)\n",
    "    y_center = reference_anchors[:, 1:2] + (anchor_predictions[..., 1:2] / scale)\n",
    "\n",
    "    w = anchor_predictions[..., 2:3] / scale\n",
    "    h = anchor_predictions[..., 3:4] / scale\n",
    "\n",
    "    x_min = x_center - w / 2.\n",
    "    y_min = y_center - h / 2.\n",
    "    x_max = x_center + w / 2.\n",
    "    y_max = y_center + h / 2.\n",
    "\n",
    "    offset_boxes = tf.concat([x_min, y_min, x_max, y_max], axis=-1)\n",
    "\n",
    "    filtered_true_coords = tf.boolean_mask(true_coords, faces_mask_bool)\n",
    "    filtered_pred_coords = tf.boolean_mask(offset_boxes, faces_mask_bool)\n",
    "\n",
    "    detection_loss = huber_loss(filtered_true_coords, filtered_pred_coords)\n",
    "\n",
    "    loss = tf.math.reduce_mean(detection_loss) * 150 + (background_loss * 35) + (positive_loss * 35)\n",
    "\n",
    "    return loss, filtered_pred_coords, filtered_true_coords, positive_class_predictions, background_class_predictions"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "18fc2f8316ac987a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:46:58.954751Z",
     "start_time": "2026-01-07T16:46:58.929952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "small_boxes = np.linspace(0.03125, 0.96875, 16, endpoint=True, dtype=np.float32) # 16x16\n",
    "big_boxes = np.linspace(0.0625, .9375, 8, endpoint=True, dtype=np.float32) # 8x8\n",
    "list_of_boxes = [small_boxes, big_boxes]\n",
    "\n",
    "small_x = tf.tile(tf.repeat(small_boxes, repeats=2), [16]) # x\n",
    "small_y = tf.repeat(small_boxes, repeats=32)\n",
    "\n",
    "small = tf.stack([small_x, small_y], axis=1)\n",
    "\n",
    "big_x = tf.tile(tf.repeat(big_boxes, repeats=6), [8]) # x\n",
    "big_y = tf.repeat(big_boxes, repeats=48)\n",
    "\n",
    "big = tf.stack([big_x, big_y], axis=1)\n",
    "\n",
    "reference_anchors = tf.concat([small, big], axis=0)\n",
    "\n",
    "def mean_iou(true_boxes, pred_boxes, return_mean=True):\n",
    "    x_min = tf.math.maximum(true_boxes[..., 0], pred_boxes[..., 0])\n",
    "    y_min = tf.math.maximum(true_boxes[..., 1], pred_boxes[..., 1])\n",
    "    x_max = tf.math.minimum(true_boxes[..., 2], pred_boxes[..., 2])\n",
    "    y_max = tf.math.minimum(true_boxes[..., 3], pred_boxes[..., 3])\n",
    "\n",
    "    overlap_area = tf.math.maximum(0.0, x_max - x_min + 1) * tf.math.maximum(0.0, y_max - y_min + 1)\n",
    "\n",
    "    true_boxes_area = (true_boxes[..., 2] - true_boxes[..., 0] + 1) * (true_boxes[..., 3] - true_boxes[..., 1] + 1)\n",
    "\n",
    "    predicted_boxes_area = (pred_boxes[..., 2] - pred_boxes[..., 0] + 1) * (pred_boxes[..., 3] - pred_boxes[..., 1] + 1)\n",
    "\n",
    "    union_area = (true_boxes_area + predicted_boxes_area - overlap_area)\n",
    "\n",
    "    if return_mean:\n",
    "        return tf.math.reduce_mean(overlap_area / union_area)\n",
    "    else:\n",
    "        return overlap_area / union_area\n",
    "\n",
    "def average_maximum_sup(class_predictions, anchor_predictions, reference_anchors, scale=128):\n",
    "    # class_predictions # B, 896, 1\n",
    "    # anchor_predictions # B, 896, 4\n",
    "\n",
    "    x_center = reference_anchors[:, 0:1] + (anchor_predictions[..., 0:1] / scale) # 8, 896, 1\n",
    "    y_center = reference_anchors[:, 1:2] + (anchor_predictions[..., 1:2] / scale) # 8, 896, 1\n",
    "\n",
    "    w = anchor_predictions[..., 2:3] / scale # B, 896, 1\n",
    "    h = anchor_predictions[..., 3:4] / scale # B, 896, 1\n",
    "\n",
    "    y_min = y_center - h / 2.  # ymin\n",
    "    x_min = x_center - w / 2.  # xmin\n",
    "    y_max = y_center + h / 2.  # ymax\n",
    "    x_max = x_center + w / 2.  # xmax\n",
    "\n",
    "    offset_boxes = tf.concat([x_min, y_min, x_max, y_max], axis=-1) # B, 896, 4\n",
    "\n",
    "    class_predictions = tf.squeeze(class_predictions, axis=-1)\n",
    "\n",
    "    mask = class_predictions >= 0.75 # 0.75 B, 896\n",
    "\n",
    "    final_detections = [] # final shape B, num_image_detections, 5 where num_image_detections can vary by image\n",
    "\n",
    "    for index, image_detections in enumerate(mask): # each 896 for each image\n",
    "        # print(image_detections)\n",
    "        num_image_detections = tf.keras.backend.sum(tf.dtypes.cast(image_detections, tf.int32))\n",
    "        # print(image_detections.shape)\n",
    "        # print(num_image_detections)\n",
    "\n",
    "        if num_image_detections == 0:\n",
    "            final_detections.append([])\n",
    "        else:\n",
    "            filtered_boxes = tf.boolean_mask(offset_boxes[index], image_detections)\n",
    "            filtered_scores = tf.boolean_mask(class_predictions[index], image_detections)\n",
    "\n",
    "            final_detections.append(tf.concat([tf.expand_dims(filtered_scores, axis=-1), filtered_boxes], axis=-1)) # num_image_detections, 5\n",
    "\n",
    "    output_detections = []\n",
    "\n",
    "    for image_detections in final_detections: # for each image B\n",
    "        # num_image_detections, 5\n",
    "        if image_detections == []:\n",
    "            output_detections.append([])\n",
    "            continue\n",
    "\n",
    "        remaining = tf.argsort(image_detections[:, 0], axis=0, direction='DESCENDING') # num_image_detections\n",
    "\n",
    "        faces = []\n",
    "\n",
    "        while remaining.shape[0] > 0:\n",
    "            detection = image_detections[remaining[0]]\n",
    "            first_box = detection[1:] # 1, 4\n",
    "            other_boxes = tf.gather(image_detections, remaining)[:, 1:] # 4, 4\n",
    "\n",
    "            ious = mean_iou(np.array(first_box) * 128.0, np.array(other_boxes) * 128.0, return_mean=False) # num_image_detections\n",
    "\n",
    "            # mask = ious > 0.3 # 4\n",
    "\n",
    "            overlapping = tf.boolean_mask(remaining, ious > 0.3)\n",
    "            remaining = tf.boolean_mask(remaining, ious <= 0.3) # When all false, returns shape 0\n",
    "            # The remaining boxes should belong to a different face\n",
    "\n",
    "            if overlapping.shape[0] > 1:\n",
    "                overlapping_boxes = tf.gather(image_detections, overlapping)\n",
    "                coordinates = overlapping_boxes[:, 1:] # overlapping, 4\n",
    "                scores = overlapping_boxes[:, 0:1] # overlapping, 1\n",
    "                total_score = tf.keras.backend.sum(scores)\n",
    "\n",
    "                weighted_boxes = tf.keras.backend.sum((coordinates * scores), axis=0) / total_score # overlapping, 4\n",
    "                weighted_score = total_score / overlapping.shape[0] # overlapping, 1\n",
    "\n",
    "                weighted_score = tf.reshape(weighted_score, (1,))\n",
    "\n",
    "                weighted_detection = tf.concat([weighted_score, weighted_boxes], axis=0) # overlapping, 5\n",
    "\n",
    "                faces.append(weighted_detection)\n",
    "\n",
    "            else:\n",
    "                faces.append(detection)\n",
    "\n",
    "        output_detections.append(faces)\n",
    "\n",
    "        return output_detections, final_detections\n",
    "\n",
    "\n",
    "def order_big_anchors_randomly(big_anchors):\n",
    "  big_anchor_list = []\n",
    "  total_boxes = 30\n",
    "\n",
    "  for batch_element in big_anchors: # B, 8, 8, 5\n",
    "    box_index = tf.random.uniform([], 0, 6, dtype=tf.dtypes.int32) * 5 # 0, 5, 10, 15, 20, 25\n",
    "\n",
    "    if box_index == 0:\n",
    "      right_box = tf.zeros((8, 8, total_boxes - 5))\n",
    "\n",
    "      anchor_tensor = tf.concat([batch_element, right_box], axis=-1)\n",
    "      big_anchor_list.append(anchor_tensor)\n",
    "\n",
    "    elif box_index == 25:\n",
    "      left_box = tf.zeros((8, 8, total_boxes - 5))\n",
    "\n",
    "      anchor_tensor = tf.concat([left_box, batch_element], axis=-1)\n",
    "      big_anchor_list.append(anchor_tensor)\n",
    "\n",
    "    else:\n",
    "      left_box = tf.zeros((8, 8, box_index))\n",
    "      right_box = tf.zeros((8, 8, total_boxes - box_index - 5))\n",
    "\n",
    "      anchor_tensor = tf.concat([left_box, batch_element, right_box], axis=-1)\n",
    "      big_anchor_list.append(anchor_tensor)\n",
    "\n",
    "\n",
    "  big_anchor_list = tf.stack(big_anchor_list, axis=0) # B, 8, 8, 30\n",
    "\n",
    "  return big_anchor_list\n",
    "\n",
    "\n",
    "def order_small_anchors_randomly(small_anchors):\n",
    "  small_anchor_list = []\n",
    "\n",
    "  for batch_element in small_anchors: # B, 16, 16, 5\n",
    "    box_index = tf.random.uniform([], 0, 2, dtype=tf.dtypes.int32) * 5 # 0, 5\n",
    "\n",
    "    if box_index == 0:\n",
    "      right_box = tf.zeros((16, 16, 5))\n",
    "\n",
    "      anchor_tensor = tf.concat([batch_element, right_box], axis=-1)\n",
    "      small_anchor_list.append(anchor_tensor)\n",
    "\n",
    "    else:\n",
    "      left_box = tf.zeros((16, 16, 5))\n",
    "\n",
    "      anchor_tensor = tf.concat([left_box, batch_element], axis=-1)\n",
    "      small_anchor_list.append(anchor_tensor)\n",
    "\n",
    "  small_anchor_list = tf.stack(small_anchor_list, axis=0) # B, 16, 16, 10\n",
    "\n",
    "  return small_anchor_list\n"
   ],
   "id": "b743914657ae7dbd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DataSet",
   "id": "5190a0c8ea21451f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:29.948433Z",
     "start_time": "2026-01-07T16:46:58.958395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "def get_vertical_new_axes(params, final_image_size=128):\n",
    "    boxes_width, boxes_height, height, width, vertical_smaller, horizontal_smaller, space_up, space_down, space_left, space_right = params\n",
    "    # First find horizontal accomodation\n",
    "    if horizontal_smaller == \"left\":\n",
    "        min_width = 0\n",
    "        while True:\n",
    "            # space we take from the left size\n",
    "            box_left_space = np.random.randint(min_width, space_left + 1)\n",
    "            current_box_size = box_left_space + boxes_width\n",
    "\n",
    "            if current_box_size >= final_image_size:\n",
    "                min_width_needed = 0\n",
    "            else:\n",
    "                min_width_needed = (final_image_size - current_box_size)\n",
    "\n",
    "            if boxes_height >= current_box_size and boxes_height >= final_image_size:\n",
    "                min_width_needed = (boxes_height - current_box_size)\n",
    "\n",
    "            if space_right >= min_width_needed:\n",
    "                break\n",
    "            else:\n",
    "                min_width = box_left_space\n",
    "\n",
    "        # space we take from the right size\n",
    "        box_right_space = np.random.randint(min_width_needed, space_right + 1)\n",
    "        image_horizontal_space = box_left_space + boxes_width + box_right_space\n",
    "\n",
    "    else:\n",
    "        min_width = 0\n",
    "        while True:\n",
    "            box_right_space = np.random.randint(min_width, space_right + 1) # check what happen when i have the same number in both parameters\n",
    "            current_box_size = box_right_space + boxes_width\n",
    "            min_width_needed = 0 if current_box_size >= final_image_size else (final_image_size - current_box_size)\n",
    "\n",
    "            if current_box_size >= final_image_size:\n",
    "                min_width_needed = 0\n",
    "            else:\n",
    "                min_width_needed = (final_image_size - current_box_size)\n",
    "\n",
    "            if boxes_height >= current_box_size and boxes_height >= final_image_size:\n",
    "                min_width_needed = (boxes_height - current_box_size)\n",
    "\n",
    "            if space_left >= min_width_needed:\n",
    "                break\n",
    "            else:\n",
    "                min_width = box_right_space\n",
    "\n",
    "        # space we take from the left size\n",
    "        box_left_space = np.random.randint(min_width_needed, space_left + 1)\n",
    "        image_horizontal_space = box_right_space + boxes_width + box_left_space\n",
    "\n",
    "    # Then find vertical accomodation\n",
    "    if vertical_smaller == \"up\":\n",
    "        min_height = 0\n",
    "        max_height = space_up + 1\n",
    "\n",
    "        while True:\n",
    "            box_up_space = np.random.randint(min_height, max_height)\n",
    "            current_box_size = box_up_space + boxes_height\n",
    "            min_height_needed = image_horizontal_space - current_box_size # -10\n",
    "\n",
    "            if current_box_size > image_horizontal_space:\n",
    "                max_height = box_up_space\n",
    "                continue\n",
    "\n",
    "            if space_down >= min_height_needed:\n",
    "                break\n",
    "            else:\n",
    "                # we need more space in the up size\n",
    "                min_height = box_up_space\n",
    "\n",
    "            # space we take from the down side\n",
    "        box_down_space = min_height_needed #np.random.randint(min_height_needed, image_horizontal_space)\n",
    "        image_vertical_space = box_up_space + boxes_height + box_down_space\n",
    "\n",
    "    else:\n",
    "        min_height = 0\n",
    "        max_height = space_down + 1\n",
    "\n",
    "        while True:\n",
    "            box_down_space = np.random.randint(min_height, max_height)\n",
    "            current_box_size = box_down_space + boxes_height\n",
    "            min_height_needed = image_horizontal_space - current_box_size\n",
    "\n",
    "            if current_box_size > image_horizontal_space:\n",
    "                # our current box height is higher than the wider box\n",
    "                max_height = box_down_space\n",
    "                continue\n",
    "\n",
    "            if space_up >= min_height_needed:\n",
    "                break\n",
    "            else:\n",
    "                # we need more space in the up size\n",
    "                min_height = box_down_space\n",
    "\n",
    "        # space we take from the left size\n",
    "        box_up_space = min_height_needed #np.random.randint(min_height_needed, image_horizontal_space)\n",
    "        image_vertical_space = box_down_space + boxes_height + box_up_space\n",
    "\n",
    "    return image_vertical_space, image_horizontal_space, box_up_space, box_down_space, box_right_space, box_left_space\n",
    "\n",
    "def get_horizontal_new_axes(params, final_image_size=128):\n",
    "    boxes_width, boxes_height, height, width, vertical_smaller, horizontal_smaller, space_up, space_down, space_left, space_right = params\n",
    "    borders = 0\n",
    "    if boxes_width > height:\n",
    "        diff = width - height\n",
    "\n",
    "        if (diff // 2) < 100:\n",
    "            borders = (diff // 2) - 2\n",
    "        else:\n",
    "            borders = 100\n",
    "\n",
    "        space_up += borders\n",
    "        space_down += borders\n",
    "\n",
    "    # First find vertical acomodation\n",
    "    if vertical_smaller == \"up\":\n",
    "        min_height = 0\n",
    "\n",
    "        while True:\n",
    "            box_up_space = np.random.randint(min_height, space_up + 1)\n",
    "            current_box_size = box_up_space + boxes_height\n",
    "\n",
    "            # If the current space used is greater than the final image size\n",
    "            if current_box_size >= final_image_size: # final_image_size = 128\n",
    "                min_height_needed = 0\n",
    "            else:\n",
    "                min_height_needed = (final_image_size - current_box_size)\n",
    "\n",
    "            # If our current space usage is less than the space used in the width\n",
    "            # we need more space\n",
    "            if boxes_width >= current_box_size and boxes_width >= final_image_size:\n",
    "                min_height_needed = (boxes_width - current_box_size)\n",
    "\n",
    "            if space_down >= min_height_needed:\n",
    "                break\n",
    "            else:\n",
    "                min_height = box_up_space # use more space\n",
    "\n",
    "        # space we take from the down side\n",
    "        box_down_space = np.random.randint(min_height_needed, space_down + 1)\n",
    "        image_vertical_space = box_up_space + boxes_height + box_down_space\n",
    "\n",
    "    else:\n",
    "        min_height = 0\n",
    "\n",
    "        while True:\n",
    "            box_down_space = np.random.randint(min_height, space_down + 1)\n",
    "            current_box_size = box_down_space + boxes_height\n",
    "\n",
    "            if current_box_size >= final_image_size: # final_image_size = 128\n",
    "                min_height_needed = 0\n",
    "            else:\n",
    "                min_height_needed = (final_image_size - current_box_size)\n",
    "\n",
    "            if boxes_width >= current_box_size and boxes_width >= final_image_size:\n",
    "                min_height_needed = (boxes_width - current_box_size)\n",
    "\n",
    "            if space_up >= min_height_needed:\n",
    "                break\n",
    "            else:\n",
    "                min_height = box_down_space # use more space\n",
    "\n",
    "        # space we take from the up side\n",
    "        box_up_space = np.random.randint(min_height_needed, space_up + 1)\n",
    "        image_vertical_space = box_down_space + boxes_height + box_up_space\n",
    "\n",
    "    # Then horizontal accomodation\n",
    "    if horizontal_smaller == \"left\":\n",
    "        min_width = 0\n",
    "        max_width = space_left + 1\n",
    "\n",
    "        while True:\n",
    "            # space we take from the left size\n",
    "            box_left_space = np.random.randint(min_width, max_width)\n",
    "            current_box_size = box_left_space + boxes_width\n",
    "            min_width_needed = image_vertical_space - current_box_size # -10\n",
    "\n",
    "            if current_box_size > image_vertical_space:\n",
    "                max_width = box_left_space\n",
    "                continue\n",
    "\n",
    "            if space_right >= min_width_needed:\n",
    "                break\n",
    "            else:\n",
    "                # we need more space in the up size\n",
    "                min_width = box_left_space\n",
    "\n",
    "        # space we take from the right size\n",
    "        box_right_space = min_width_needed\n",
    "        image_horizontal_space = box_left_space + boxes_width + box_right_space\n",
    "\n",
    "    else:\n",
    "        min_width = 0\n",
    "        max_width = space_right + 1\n",
    "\n",
    "        while True:\n",
    "            box_right_space = np.random.randint(min_width, max_width)\n",
    "            current_box_size = box_right_space + boxes_width\n",
    "            min_width_needed = image_vertical_space - current_box_size\n",
    "\n",
    "            if current_box_size > image_vertical_space:\n",
    "                max_width = box_right_space\n",
    "                continue\n",
    "\n",
    "            if space_left >= min_width_needed:\n",
    "                break\n",
    "            else:\n",
    "                min_width = box_right_space\n",
    "\n",
    "        # space we take from the left size\n",
    "        box_left_space = min_width_needed\n",
    "        image_horizontal_space = box_right_space + boxes_width + box_left_space\n",
    "\n",
    "    return image_vertical_space, image_horizontal_space, box_up_space, box_down_space, box_right_space, box_left_space, borders\n",
    "\n",
    "\n",
    "def get_box_sizes(boxes):\n",
    "    max_box_height = 0\n",
    "    min_box_height = 99999\n",
    "\n",
    "    max_box_width = 0\n",
    "    min_box_width = 99999\n",
    "\n",
    "    for box in boxes: # For each face in image\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "\n",
    "        x2 = (box[0] + box[2])\n",
    "        y2 = (box[1] + box[3])\n",
    "\n",
    "        current_min_box_height = y1\n",
    "        current_max_box_height = y2\n",
    "\n",
    "        if min_box_height > current_min_box_height:\n",
    "            min_box_height = current_min_box_height\n",
    "\n",
    "        if current_max_box_height > max_box_height:\n",
    "            max_box_height = current_max_box_height\n",
    "\n",
    "        current_min_box_width = x1\n",
    "        current_max_box_width = x2\n",
    "\n",
    "        if min_box_width > current_min_box_width:\n",
    "            min_box_width = current_min_box_width\n",
    "\n",
    "        if current_max_box_width > max_box_width:\n",
    "            max_box_width = current_max_box_width\n",
    "\n",
    "    return max_box_height, min_box_height, max_box_width, min_box_width\n",
    "\n",
    "def get_random_image(img, boxes):\n",
    "    H, W, C = img.shape\n",
    "\n",
    "    # Get the corners of all the boxes to know where we can cut the image\n",
    "    max_box_height, min_box_height, max_box_width, min_box_width = get_box_sizes(boxes)\n",
    "\n",
    "    boxes_height = max_box_height - min_box_height\n",
    "    boxes_width = max_box_width - min_box_width\n",
    "\n",
    "    space_up = min_box_height\n",
    "    space_down = H - max_box_height\n",
    "\n",
    "    space_left = min_box_width\n",
    "    space_right = W - max_box_width\n",
    "\n",
    "    horizontal_smaller = \"left\" if space_right > space_left else \"right\"\n",
    "    vertical_smaller = \"up\" if space_down > space_up else \"down\"\n",
    "\n",
    "    small_direction = \"horizontal\" if H >= W else \"vertical\"\n",
    "\n",
    "    params = [boxes_width, boxes_height, H, W, vertical_smaller, horizontal_smaller, space_up, space_down, space_left, space_right]\n",
    "\n",
    "    if small_direction == \"vertical\":\n",
    "        image_vertical_space, image_horizontal_space, box_up_space, box_down_space, box_right_space, box_left_space, borders = get_horizontal_new_axes(params)\n",
    "    else:\n",
    "        image_vertical_space, image_horizontal_space, box_up_space, box_down_space, box_right_space, box_left_space = get_vertical_new_axes(params)\n",
    "        borders = 0\n",
    "\n",
    "    image_x1 = min_box_width - box_left_space\n",
    "    image_x2 = max_box_width + box_right_space\n",
    "\n",
    "    image_y1 = (min_box_height + borders) - box_up_space\n",
    "    image_y2 = (max_box_height + borders) + box_down_space\n",
    "\n",
    "    if borders > 0:\n",
    "        new_img = np.zeros([H + (borders * 2), W, C], dtype=np.uint8)\n",
    "        H, W, C = new_img.shape\n",
    "\n",
    "        new_img[borders:H - borders, :, :] = img\n",
    "        img = new_img\n",
    "\n",
    "    resized = img[image_y1:image_y2, image_x1:image_x2, :]\n",
    "\n",
    "    H, W, C = resized.shape\n",
    "\n",
    "    try:\n",
    "        resized = cv2.resize(resized, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "    except:\n",
    "        print(resized.shape)\n",
    "        print(image_x1, image_y1, image_x2, image_y2)\n",
    "        print(borders)\n",
    "        print(min_box_height)\n",
    "        print(max_box_height, \"max_box_height\")\n",
    "        print(box_down_space, \"box_down_space\")\n",
    "\n",
    "        raise Exception(\"Image error\")\n",
    "\n",
    "    new_boxes = []\n",
    "\n",
    "    for box in boxes: # For each face in image\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "\n",
    "        x1 = (x1 - image_x1)\n",
    "        y1 = ((y1 + borders) - image_y1)\n",
    "\n",
    "        x2 = (x1 + box[2]) / W\n",
    "        y2 = (y1 + box[3]) / H\n",
    "\n",
    "        x1 = x1 / W\n",
    "        y1 = y1 / H\n",
    "\n",
    "        new_boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    return resized, new_boxes\n",
    "\n",
    "\n",
    "images_paths = []\n",
    "boxes_dict = {}\n",
    "\n",
    "df = pd.read_csv(\"data_files/fixed_images.csv\", index_col=0)\n",
    "face_df = df[['group', 'image_path', 'x1', 'y1', 'w', 'h']]\n",
    "\n",
    "for img_path, indices in face_df.groupby(\"image_path\").groups.items():\n",
    "    selected = face_df.loc[indices]\n",
    "    group = selected.values[0][0]\n",
    "\n",
    "    original_img_path = f\"face_dataset/{img_path}\"\n",
    "    original_img = cv2.imread(original_img_path)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if group == \"None\":\n",
    "        img_path = img_path.replace(\"/\", \"-\")\n",
    "    else:\n",
    "        img_path = img_path.split(\"/\")[-1]\n",
    "\n",
    "    img_path = f\"created_dataset/set-1-{img_path}\"\n",
    "\n",
    "    boxes = selected.values[:, 2:]\n",
    "\n",
    "    images_paths.append(img_path)\n",
    "    # print(original_img, boxes)\n",
    "    resized, new_boxes = get_random_image(original_img, boxes)\n",
    "\n",
    "    boxes_dict[img_path] = []\n",
    "\n",
    "    for box in new_boxes:\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "\n",
    "        x2 = box[2]\n",
    "        y2 = box[3]\n",
    "\n",
    "        boxes_dict[img_path].append(np.array([x1, y1, x2, y2], dtype=np.float32))\n",
    "\n",
    "    cv2.imwrite(img_path, resized)\n",
    "\n",
    "    if len(images_paths) % 500 == 0:\n",
    "        print(len(images_paths))\n",
    "\n",
    "for img_path, indices in face_df.groupby(\"image_path\").groups.items():\n",
    "    selected = face_df.loc[indices]\n",
    "    group = selected.values[0][0]\n",
    "\n",
    "    original_img_path = f\"face_dataset/{img_path}\"\n",
    "    original_img = cv2.imread(original_img_path)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if group == \"None\":\n",
    "        img_path = img_path.replace(\"/\", \"-\")\n",
    "    else:\n",
    "        img_path = img_path.split(\"/\")[-1]\n",
    "\n",
    "    img_path = f\"created_dataset/set-2-{img_path}\"\n",
    "\n",
    "    boxes = selected.values[:, 2:]\n",
    "\n",
    "    images_paths.append(img_path)\n",
    "\n",
    "    resized, new_boxes = get_random_image(original_img, boxes)\n",
    "\n",
    "    boxes_dict[img_path] = []\n",
    "\n",
    "    for box in new_boxes:\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "\n",
    "        x2 = box[2]\n",
    "        y2 = box[3]\n",
    "\n",
    "        boxes_dict[img_path].append(np.array([x1, y1, x2, y2], dtype=np.float32))\n",
    "\n",
    "    cv2.imwrite(img_path, resized)\n",
    "\n",
    "    if len(images_paths) % 500 == 0:\n",
    "        print(len(images_paths))\n",
    "\n",
    "# print(len(images_paths))\n",
    "\n",
    "np.save(\"data_files/images_paths.npy\", np.array(images_paths))\n",
    "np.save(\"data_files/boxes_dict.npy\", boxes_dict)"
   ],
   "id": "211627bacf48daaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:29.960260Z",
     "start_time": "2026-01-07T16:47:29.953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def compute_iou(box, anchor_box):\n",
    "    x_min = np.maximum(box[0], anchor_box[0])\n",
    "    y_min = np.maximum(box[1], anchor_box[1])\n",
    "    x_max = np.minimum(box[2], anchor_box[2])\n",
    "    y_max = np.minimum(box[3], anchor_box[3])\n",
    "\n",
    "    overlap_area = np.maximum(0.0, x_max - x_min + 1) * np.maximum(0.0, y_max - y_min + 1)\n",
    "\n",
    "    true_boxes_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    anchor_boxes_area = (anchor_box[2] - anchor_box[0] + 1) * (anchor_box[3] - anchor_box[1] + 1)\n",
    "\n",
    "    union_area = float(true_boxes_area + anchor_boxes_area - overlap_area)\n",
    "\n",
    "    return overlap_area / union_area\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [128, 128])\n",
    "\n",
    "    return img\n",
    "\n",
    "def create_boxes(boxes, horizontal):\n",
    "    indices_list = {\"small\": [], \"big\": []}\n",
    "    coords_list = {\"small\": [], \"big\": []}\n",
    "\n",
    "    box_type = {\"0.03125\": \"small\", \"0.0625\": \"big\"}\n",
    "\n",
    "    for box in boxes: # For each face in image\n",
    "        box_x1 = box[0]\n",
    "        box_y1 = box[1]\n",
    "\n",
    "        box_x2 = box[2]\n",
    "        box_y2 = box[3]\n",
    "\n",
    "    if horizontal:\n",
    "        w = box_x2 - box_x1\n",
    "        box_x1 = (1 - box_x1) - w\n",
    "        box_x2 = (box_x1 + w)\n",
    "\n",
    "    face_box = np.array([box_x1, box_y1, box_x2, box_y2])\n",
    "\n",
    "    iou = 0.01\n",
    "\n",
    "    for box_values in list_of_boxes:\n",
    "        best_indices = []\n",
    "        best_coords = []\n",
    "\n",
    "        for y_index, y_coord in enumerate(box_values): # y space\n",
    "            for x_index, x_coord in enumerate(box_values): # x space we move throughout x, y keep its value and x change\n",
    "                x1 = x_coord - box_values[0]\n",
    "                y1 = y_coord - box_values[0]\n",
    "\n",
    "                x2 = x_coord + box_values[0]\n",
    "                y2 = y_coord + box_values[0]\n",
    "\n",
    "                anchor_box = np.array([x1, y1, x2, y2])\n",
    "\n",
    "                current_iou = compute_iou(face_box * 128.0, anchor_box * 128.0)\n",
    "\n",
    "                if current_iou >= iou:\n",
    "                    iou = current_iou\n",
    "\n",
    "                    best_indices.append([x_index, y_index])\n",
    "                    best_coords.append(face_box)\n",
    "\n",
    "        indices_list[box_type[str(box_values[0])]].extend(best_indices)\n",
    "        coords_list[box_type[str(box_values[0])]].extend(best_coords)\n",
    "\n",
    "    big_anchor = np.zeros((8, 8, 5))\n",
    "\n",
    "    indices = indices_list[\"big\"]\n",
    "    coords = coords_list[\"big\"]\n",
    "\n",
    "    for index in range(len(indices)):\n",
    "        x_index, y_index = indices[index]\n",
    "        box = coords[index].tolist()\n",
    "        big_anchor[y_index, x_index] = [1, *box]\n",
    "\n",
    "    small_anchor = np.zeros((16, 16, 5))\n",
    "\n",
    "    indices = indices_list[\"small\"]\n",
    "    coords = coords_list[\"small\"]\n",
    "\n",
    "    for index in range(len(indices)):\n",
    "        x_index, y_index = indices[index]\n",
    "        box = coords[index].tolist()\n",
    "        small_anchor[y_index, x_index] = [1, *box]\n",
    "\n",
    "    return big_anchor, small_anchor\n",
    "\n",
    "def get_boxes_from_dictionary(image_path, horizontal):\n",
    "    key = image_path.numpy().decode(\"utf-8\")\n",
    "\n",
    "    boxes = boxes_dict[key]\n",
    "\n",
    "    horizontal = bool(horizontal)\n",
    "\n",
    "    big_anchor, small_anchor = create_boxes(boxes, horizontal)\n",
    "\n",
    "    return big_anchor, small_anchor\n",
    "\n",
    "def load_train_image(image_path):\n",
    "    # # tf.print(\"path:\", image_path)\n",
    "    img = load_image(image_path)\n",
    "\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        img = tf.image.random_brightness(img, 0.2)\n",
    "\n",
    "    horizontal = tf.random.uniform([]) > 0.5\n",
    "\n",
    "    if horizontal:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "\n",
    "    big_anchor, small_anchor = tf.py_function(get_boxes_from_dictionary, [image_path, horizontal], [tf.float32, tf.float32])\n",
    "\n",
    "    return img, big_anchor, small_anchor, image_path, horizontal\n",
    "\n",
    "def load_test_image(image_path):\n",
    "    # # tf.print(\"path:\", image_path)\n",
    "    img = load_image(image_path)\n",
    "    big_anchor, small_anchor = tf.py_function(get_boxes_from_dictionary, [image_path, False], [tf.float32, tf.float32])\n",
    "\n",
    "    return image_path, img, big_anchor, small_anchor"
   ],
   "id": "72765a4e22d589af",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:30.082694Z",
     "start_time": "2026-01-07T16:47:29.965893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((images_paths))\n",
    "test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_generator = test_dataset.batch(32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((images_paths))\n",
    "train_dataset = train_dataset.shuffle(len(images_paths))\n",
    "train_dataset = train_dataset.map(load_train_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_generator = train_dataset.batch(32)"
   ],
   "id": "3cce14e4e3d00f10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mc/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "89e884c4",
   "metadata": {},
   "source": [
    "\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "af7e33dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:30.574121Z",
     "start_time": "2026-01-07T16:47:30.096675Z"
    }
   },
   "source": [
    "continue_training = True\n",
    "images_paths = np.load('data_files/images_paths.npy')\n",
    "boxes_dict = np.load('data_files/boxes_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "train_steps = int(len(images_paths) / batch_size) +1\n",
    "model = BlazeModel()\n",
    "dummy_input = tf.ones((1, 128, 128, 3))\n",
    "model(dummy_input) # On fait un passage à vide\n",
    "\n",
    "\n",
    "print(\"Le schéma du modèle a été enregistré sous 'model_debug.png'\")\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "\n",
    "if continue_training:\n",
    "    print(\"loading training checkpoint: \")\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir))\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir))\n",
    "\n",
    "positive_accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "background_accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "positive_accuracy_results = []\n",
    "background_accuracy_results = []\n",
    "loss_results = []\n",
    "iou_results = []\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le schéma du modèle a été enregistré sous 'model_debug.png'\n",
      "loading training checkpoint: \n",
      "None\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "4f9d8660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:30.590300Z",
     "start_time": "2026-01-07T16:47:30.587598Z"
    }
   },
   "source": [
    "@tf.function\n",
    "def train_step(images, big_anchors, small_anchors, reference_anchors, model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        class_predictions, anchor_predictions = model(images, training=True)\n",
    "        loss, filtered_pred_coords, filtered_true_coords, positive_class_predictions, background_class_predictions = compute_loss(class_predictions, anchor_predictions, big_anchors, small_anchors, reference_anchors)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, filtered_pred_coords, filtered_true_coords, positive_class_predictions, background_class_predictions"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "388edb31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:47:30.597187Z",
     "start_time": "2026-01-07T16:47:30.593326Z"
    }
   },
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        batch_time = time.time()\n",
    "        epoch_time = time.time()\n",
    "        step = 0\n",
    "        epoch_count = f\"0{epoch + 1}/{epochs}\" if epoch < 9 else f\"{epoch + 1}/{epochs}\"\n",
    "\n",
    "        positive_accuracy_metric.reset_state()\n",
    "        background_accuracy_metric.reset_state()\n",
    "\n",
    "        for images, big_anchors, small_anchors, _, _ in train_generator:\n",
    "            small_anchors = order_small_anchors_randomly(small_anchors)\n",
    "            big_anchors = order_big_anchors_randomly(big_anchors)\n",
    "\n",
    "            loss, filtered_pred_coords, filtered_true_coords, positive_class_predictions, background_class_predictions = train_step(images, big_anchors, small_anchors, reference_anchors, model)\n",
    "\n",
    "            positive_accuracy_metric.update_state(tf.ones_like(positive_class_predictions), positive_class_predictions)\n",
    "            background_accuracy_metric.update_state(tf.zeros_like(background_class_predictions), background_class_predictions)\n",
    "\n",
    "            loss = float(loss)\n",
    "            step+=1\n",
    "\n",
    "            positive_accuracy = positive_accuracy_metric.result().numpy()\n",
    "            background_accuracy = background_accuracy_metric.result().numpy()\n",
    "\n",
    "            iou = mean_iou(filtered_true_coords * 128.0, filtered_pred_coords * 128.0)\n",
    "\n",
    "            iou_results.append(iou)\n",
    "            positive_accuracy_results.append(positive_accuracy)\n",
    "            background_accuracy_results.append(background_accuracy)\n",
    "            loss_results.append(loss)\n",
    "\n",
    "            print(\n",
    "                '\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
    "                '| Loss:', f\"{loss:.5f}\", '| Positive Accuracy:', f\"{positive_accuracy:.4f}\",\n",
    "                '| Background Accuracy:', f\"{background_accuracy:.4f}\",\n",
    "                '| iou:', f\"{iou:.4f}\", \"| Step Time:\", f\"{time.time() - batch_time:.2f}\", end=''\n",
    "            )\n",
    "\n",
    "            batch_time = time.time()\n",
    "\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        print(\n",
    "            '\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
    "            '| Loss:', f\"{loss:.5f}\", '| Positive Accuracy:', f\"{positive_accuracy:.4f}\",\n",
    "            '| Background Accuracy:', f\"{background_accuracy:.4f}\",\n",
    "            '| iou:', f\"{iou:.4f}\", \"| Epoch Time:\", f\"{time.time() - epoch_time:.2f}\"\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:48:15.542547Z",
     "start_time": "2026-01-07T16:47:30.601281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(epochs)\n",
    "model.save(\"weights/saved_model/face\", include_optimizer=False)"
   ],
   "id": "a12dade8bbd289c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 1/280 | Loss: 169.03868 | Positive Accuracy: 0.2984 | Background Accuracy: 0.6482 | iou: 0.0005 | Step Time: 1.13DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 2/280 | Loss: 141.30756 | Positive Accuracy: 0.3324 | Background Accuracy: 0.6524 | iou: 0.0009 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 3/280 | Loss: 136.50931 | Positive Accuracy: 0.3348 | Background Accuracy: 0.6509 | iou: 0.0009 | Step Time: 0.72DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 4/280 | Loss: 149.30785 | Positive Accuracy: 0.3389 | Background Accuracy: 0.6500 | iou: 0.0006 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 5/280 | Loss: 156.45401 | Positive Accuracy: 0.3357 | Background Accuracy: 0.6485 | iou: 0.0004 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 6/280 | Loss: 152.58434 | Positive Accuracy: 0.3322 | Background Accuracy: 0.6470 | iou: 0.0008 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 7/280 | Loss: 149.66316 | Positive Accuracy: 0.3319 | Background Accuracy: 0.6440 | iou: 0.0007 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 8/280 | Loss: 132.03278 | Positive Accuracy: 0.3392 | Background Accuracy: 0.6413 | iou: 0.0007 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 9/280 | Loss: 139.99580 | Positive Accuracy: 0.3516 | Background Accuracy: 0.6378 | iou: 0.0011 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 10/280 | Loss: 135.79031 | Positive Accuracy: 0.3595 | Background Accuracy: 0.6353 | iou: 0.0009 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 11/280 | Loss: 134.05850 | Positive Accuracy: 0.3673 | Background Accuracy: 0.6331 | iou: 0.0009 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 12/280 | Loss: 126.15192 | Positive Accuracy: 0.3723 | Background Accuracy: 0.6306 | iou: 0.0012 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 13/280 | Loss: 123.47392 | Positive Accuracy: 0.3816 | Background Accuracy: 0.6279 | iou: 0.0010 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 14/280 | Loss: 111.77149 | Positive Accuracy: 0.3919 | Background Accuracy: 0.6258 | iou: 0.0020 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 15/280 | Loss: 116.98858 | Positive Accuracy: 0.3993 | Background Accuracy: 0.6239 | iou: 0.0008 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 16/280 | Loss: 115.88495 | Positive Accuracy: 0.4080 | Background Accuracy: 0.6220 | iou: 0.0013 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 17/280 | Loss: 108.42972 | Positive Accuracy: 0.4207 | Background Accuracy: 0.6200 | iou: 0.0011 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 18/280 | Loss: 113.94191 | Positive Accuracy: 0.4260 | Background Accuracy: 0.6184 | iou: 0.0006 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 19/280 | Loss: 102.29633 | Positive Accuracy: 0.4361 | Background Accuracy: 0.6160 | iou: 0.0015 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 20/280 | Loss: 110.24503 | Positive Accuracy: 0.4458 | Background Accuracy: 0.6144 | iou: 0.0009 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 21/280 | Loss: 100.83511 | Positive Accuracy: 0.4537 | Background Accuracy: 0.6125 | iou: 0.0007 | Step Time: 0.77DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 22/280 | Loss: 106.22043 | Positive Accuracy: 0.4607 | Background Accuracy: 0.6107 | iou: 0.0006 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 23/280 | Loss: 99.78200 | Positive Accuracy: 0.4666 | Background Accuracy: 0.6090 | iou: 0.0017 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 24/280 | Loss: 97.22251 | Positive Accuracy: 0.4760 | Background Accuracy: 0.6079 | iou: 0.0014 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 25/280 | Loss: 99.33700 | Positive Accuracy: 0.4845 | Background Accuracy: 0.6064 | iou: 0.0005 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 26/280 | Loss: 103.59117 | Positive Accuracy: 0.4902 | Background Accuracy: 0.6052 | iou: 0.0009 | Step Time: 1.03DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 27/280 | Loss: 93.12465 | Positive Accuracy: 0.4974 | Background Accuracy: 0.6038 | iou: 0.0012 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 28/280 | Loss: 90.16679 | Positive Accuracy: 0.5064 | Background Accuracy: 0.6029 | iou: 0.0017 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 29/280 | Loss: 99.97173 | Positive Accuracy: 0.5111 | Background Accuracy: 0.6017 | iou: 0.0004 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 30/280 | Loss: 105.15172 | Positive Accuracy: 0.5156 | Background Accuracy: 0.6007 | iou: 0.0005 | Step Time: 0.98DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 31/280 | Loss: 98.70211 | Positive Accuracy: 0.5202 | Background Accuracy: 0.6001 | iou: 0.0015 | Step Time: 0.93DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 32/280 | Loss: 104.65030 | Positive Accuracy: 0.5244 | Background Accuracy: 0.5991 | iou: 0.0010 | Step Time: 0.79DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 33/280 | Loss: 98.97816 | Positive Accuracy: 0.5282 | Background Accuracy: 0.5985 | iou: 0.0034 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 34/280 | Loss: 83.67590 | Positive Accuracy: 0.5341 | Background Accuracy: 0.5978 | iou: 0.0007 | Step Time: 0.80DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 35/280 | Loss: 94.60186 | Positive Accuracy: 0.5373 | Background Accuracy: 0.5970 | iou: 0.0023 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 36/280 | Loss: 93.04078 | Positive Accuracy: 0.5399 | Background Accuracy: 0.5962 | iou: 0.0031 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 37/280 | Loss: 91.17419 | Positive Accuracy: 0.5427 | Background Accuracy: 0.5958 | iou: 0.0017 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 38/280 | Loss: 94.49031 | Positive Accuracy: 0.5458 | Background Accuracy: 0.5955 | iou: 0.0011 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 39/280 | Loss: 90.12467 | Positive Accuracy: 0.5498 | Background Accuracy: 0.5951 | iou: 0.0012 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 40/280 | Loss: 85.46497 | Positive Accuracy: 0.5531 | Background Accuracy: 0.5949 | iou: 0.0033 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 41/280 | Loss: 83.96249 | Positive Accuracy: 0.5578 | Background Accuracy: 0.5948 | iou: 0.0015 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 42/280 | Loss: 83.57242 | Positive Accuracy: 0.5628 | Background Accuracy: 0.5949 | iou: 0.0010 | Step Time: 0.73DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 43/280 | Loss: 89.60181 | Positive Accuracy: 0.5661 | Background Accuracy: 0.5946 | iou: 0.0012 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 44/280 | Loss: 84.28325 | Positive Accuracy: 0.5698 | Background Accuracy: 0.5948 | iou: 0.0018 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 45/280 | Loss: 83.80363 | Positive Accuracy: 0.5730 | Background Accuracy: 0.5949 | iou: 0.0023 | Step Time: 0.77DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 46/280 | Loss: 87.55681 | Positive Accuracy: 0.5753 | Background Accuracy: 0.5949 | iou: 0.0035 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 47/280 | Loss: 78.06859 | Positive Accuracy: 0.5791 | Background Accuracy: 0.5951 | iou: 0.0022 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 48/280 | Loss: 84.10681 | Positive Accuracy: 0.5818 | Background Accuracy: 0.5953 | iou: 0.0029 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 49/280 | Loss: 78.96335 | Positive Accuracy: 0.5843 | Background Accuracy: 0.5955 | iou: 0.0024 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 50/280 | Loss: 83.35249 | Positive Accuracy: 0.5869 | Background Accuracy: 0.5957 | iou: 0.0022 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 51/280 | Loss: 81.01620 | Positive Accuracy: 0.5898 | Background Accuracy: 0.5958 | iou: 0.0011 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 52/280 | Loss: 79.16252 | Positive Accuracy: 0.5918 | Background Accuracy: 0.5959 | iou: 0.0023 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 53/280 | Loss: 75.59801 | Positive Accuracy: 0.5951 | Background Accuracy: 0.5962 | iou: 0.0024 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 54/280 | Loss: 79.32391 | Positive Accuracy: 0.5975 | Background Accuracy: 0.5964 | iou: 0.0014 | Step Time: 0.74DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 55/280 | Loss: 71.94902 | Positive Accuracy: 0.6002 | Background Accuracy: 0.5968 | iou: 0.0014 | Step Time: 0.75DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 56/280 | Loss: 78.42133 | Positive Accuracy: 0.6022 | Background Accuracy: 0.5971 | iou: 0.0017 | Step Time: 0.76DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 57/280 | Loss: 77.23920 | Positive Accuracy: 0.6042 | Background Accuracy: 0.5974 | iou: 0.0031 | Step Time: 0.78DEBUG: train_step reached return\n",
      " Epoch 01/100 | Step 58/280 | Loss: 80.25928 | Positive Accuracy: 0.6058 | Background Accuracy: 0.5977 | iou: 0.0020 | Step Time: 0.76"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/nx/nf1glhf92gx00xb5ly7md0540000gn/T/ipykernel_14391/2758669989.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"weights/saved_model/face\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minclude_optimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/nx/nf1glhf92gx00xb5ly7md0540000gn/T/ipykernel_14391/3833037968.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(epochs)\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbig_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msmall_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_generator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m             \u001B[0msmall_anchors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0morder_small_anchors_randomly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msmall_anchors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0mbig_anchors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0morder_big_anchors_randomly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbig_anchors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiltered_pred_coords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiltered_true_coords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpositive_class_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackground_class_predictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbig_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msmall_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreference_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m             \u001B[0mpositive_accuracy_metric\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpositive_class_predictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpositive_class_predictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m             \u001B[0mbackground_accuracy_metric\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbackground_class_predictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackground_class_predictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    806\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    807\u001B[0m     \u001B[0;31m# Implements PolymorphicFunction.__call__.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_functions_eagerly\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    809\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf_function_call\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"eager\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 810\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    811\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    812\u001B[0m     \u001B[0;31m# Only count the statistics the first time, before initialization took\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;31m# place.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/nx/nf1glhf92gx00xb5ly7md0540000gn/T/ipykernel_14391/4291868226.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(images, big_anchors, small_anchors, reference_anchors, model)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m@\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbig_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msmall_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreference_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mclass_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manchor_predictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiltered_pred_coords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiltered_true_coords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpositive_class_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackground_class_predictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclass_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manchor_predictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbig_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msmall_anchors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreference_anchors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mgradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/layers/layer.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    975\u001B[0m                     \u001B[0;34m\"layers will not see the mask.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    976\u001B[0m                 )\n\u001B[1;32m    977\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    978\u001B[0m             \u001B[0;31m# Destroy call context if we created it\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 979\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_call_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    980\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    981\u001B[0m         \u001B[0;31m################################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    982\u001B[0m         \u001B[0;31m# 8. Add a node in the graph for symbolic calls.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/ops/operation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001B[1;32m     56\u001B[0m                 \u001B[0mcall_fn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                 \u001B[0mobject_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}.call()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m             )\n\u001B[0;32m---> 59\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0;31m# Plain flow.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0many_symbolic_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/nx/nf1glhf92gx00xb5ly7md0540000gn/T/ipykernel_14391/3268089532.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblock2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0;31m# tf.print(\"Input size Single BlazeBlock_3:\", tf.shape(x))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblock3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0;31m# tf.print(\"Input size Single BlazeBlock_4:\", tf.shape(x))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblock4\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m         \u001B[0;31m# tf.print(\"Input size Single BlazeBlock_5:\", tf.shape(x))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblock5\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/layers/layer.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    975\u001B[0m                     \u001B[0;34m\"layers will not see the mask.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    976\u001B[0m                 )\n\u001B[1;32m    977\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    978\u001B[0m             \u001B[0;31m# Destroy call context if we created it\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 979\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_call_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    980\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    981\u001B[0m         \u001B[0;31m################################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    982\u001B[0m         \u001B[0;31m# 8. Add a node in the graph for symbolic calls.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/ops/operation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001B[1;32m     56\u001B[0m                 \u001B[0mcall_fn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                 \u001B[0mobject_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}.call()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m             )\n\u001B[0;32m---> 59\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0;31m# Plain flow.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0many_symbolic_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/nx/nf1glhf92gx00xb5ly7md0540000gn/T/ipykernel_14391/2973924539.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, inputs, training)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdw_conv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/layers/layer.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    975\u001B[0m                     \u001B[0;34m\"layers will not see the mask.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    976\u001B[0m                 )\n\u001B[1;32m    977\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    978\u001B[0m             \u001B[0;31m# Destroy call context if we created it\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 979\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_call_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    980\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    981\u001B[0m         \u001B[0;31m################################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    982\u001B[0m         \u001B[0;31m# 8. Add a node in the graph for symbolic calls.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/ops/operation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001B[1;32m     56\u001B[0m                 \u001B[0mcall_fn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                 \u001B[0mobject_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}.call()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m             )\n\u001B[0;32m---> 59\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0;31m# Plain flow.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0many_symbolic_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/layers/convolutional/base_depthwise_conv.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    218\u001B[0m                 bias_shape = (1, self.depth_multiplier * input_channel) + (\n\u001B[1;32m    219\u001B[0m                     \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 ) * self.rank\n\u001B[1;32m    221\u001B[0m             \u001B[0mbias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 222\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    223\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactivation\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/ops/numpy.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(x1, x2)\u001B[0m\n\u001B[1;32m    233\u001B[0m            [10 12]], shape=(2, 2), dtype=int32)\n\u001B[1;32m    234\u001B[0m     \"\"\"\n\u001B[1;32m    235\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0many_symbolic_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    236\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mAdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msymbolic_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 237\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(x1, x2)\u001B[0m\n\u001B[1;32m    489\u001B[0m                     \u001B[0mx1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    490\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIndexedSlices\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    491\u001B[0m                 \u001B[0;31m# x2 is an IndexedSlices, densify.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m                 \u001B[0mx2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 493\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(x1, x2)\u001B[0m\n\u001B[1;32m    125\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m             \u001B[0mdata_format\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"NCHW\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m             \u001B[0mx2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 129\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias_add\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_format\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_format\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    130\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1261\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1262\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1263\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1264\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1265\u001B[0;31m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1266\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1267\u001B[0m         \u001B[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1268\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(value, bias, data_format, name)\u001B[0m\n\u001B[1;32m   3556\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3557\u001B[0m       \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"input\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3558\u001B[0m       \u001B[0mbias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"bias\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3559\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3560\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_nn_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias_add\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_format\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_format\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/au_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(value, bias, data_format, name)\u001B[0m\n\u001B[1;32m    868\u001B[0m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001B[1;32m    869\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    870\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    871\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 872\u001B[0;31m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    873\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    874\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    875\u001B[0m       return bias_add_eager_fallback(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "d3c6c596",
   "metadata": {},
   "source": [
    "# 3D Face Landmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364ab65",
   "metadata": {},
   "source": [
    "# AU classificator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936c32a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "au-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
